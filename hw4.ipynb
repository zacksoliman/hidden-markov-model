{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    "## Implementation: Hidden Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal as mv_normal\n",
    "import matplotlib.mlab as mlab\n",
    "from scipy.stats import chi2\n",
    "from matplotlib.patches import Ellipse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by loading the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = np.loadtxt('data/EMGaussian.train')\n",
    "test_data = np.loadtxt('data/EMGaussian.test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by implementing the alpha-beta recursion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alpha(A, pi):\n",
    "    pass\n",
    "\n",
    "def beta(A, pi):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a helper function that will help us compute the Gaussian pdf. This method will be used to plot the contours as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mv_gauss(X, Y, mu, cov):\n",
    "    sigma_x = np.sqrt(cov[0,0])\n",
    "    sigma_y = np.sqrt(cov[1,1])\n",
    "    sigma_xy = np.sqrt(cov[0,1])\n",
    "    \n",
    "    mu_x = mu[0]\n",
    "    mu_y = mu[1]\n",
    "    \n",
    "    return mlab.bivariate_normal(X, Y, sigma_x, sigma_y, mu_x, mu_y, sigma_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Credit to: \n",
    "# http://www.nhsilbert.net/source/2014/06/bivariate-normal-ellipse-plotting-in-python/\n",
    "def plot_cov_ellipse(cov, pos, volume=.5, ax=None, fc='none', ec=[0,0,0], a=1, lw=1):\n",
    "    \"\"\"\n",
    "    Plots an ellipse enclosing *volume* based on the specified covariance\n",
    "    matrix (*cov*) and location (*pos*). Additional keyword arguments are passed on to the \n",
    "    ellipse patch artist.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        cov : The 2x2 covariance matrix to base the ellipse on\n",
    "        pos : The location of the center of the ellipse. Expects a 2-element\n",
    "            sequence of [x0, y0].\n",
    "        volume : The volume inside the ellipse; defaults to 0.5\n",
    "        ax : The axis that the ellipse will be plotted on. Defaults to the \n",
    "            current axis.\n",
    "    \"\"\"\n",
    "\n",
    "    def eigsorted(cov):\n",
    "        vals, vecs = np.linalg.eigh(cov)\n",
    "        order = vals.argsort()[::-1]\n",
    "        return vals[order], vecs[:,order]\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    vals, vecs = eigsorted(cov)\n",
    "    theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n",
    "\n",
    "    kwrg = {'facecolor':fc, 'edgecolor':ec, 'alpha':a, 'linewidth':lw}\n",
    "\n",
    "    # Width and height are \"full\" widths, not radius\n",
    "    width, height = 2 * np.sqrt(chi2.ppf(volume,2)) * np.sqrt(vals)\n",
    "    ellip = Ellipse(xy=pos, width=width, height=height, angle=theta, **kwrg)\n",
    "\n",
    "    ax.add_artist(ellip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the K-means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class K_means:\n",
    "    \n",
    "    def __init__(self, k=4, n_dims=2):\n",
    "        self.k = k\n",
    "        self.n_dims = n_dims\n",
    "        \n",
    "    def train(self, train_data):\n",
    "        # Initialize the cluster means\n",
    "        self.means = np.random.rand(self.k, self.n_dims) * np.max(train_data, axis=0)\n",
    "        n_iter = 0\n",
    "        \n",
    "        # Matrix where each row is a z_n assignment vector associated with a data point\n",
    "        old_Z = np.zeros(shape=(train_data.shape[0], self.k))\n",
    "        self.Z = np.zeros(shape=(train_data.shape[0], self.k))\n",
    "        \n",
    "        while(not self._converged(old_Z, n_iter)):\n",
    "            old_Z = np.array(self.Z)\n",
    "            self.Z = np.zeros(shape=(train_data.shape[0], self.k))\n",
    "            \n",
    "            # First phase, we evaluate the value of the latent cluster assignment variables\n",
    "            for i, train_point in enumerate(train_data):                \n",
    "                distances = np.linalg.norm(self.means - train_point, axis=1)**2\n",
    "                self.Z[i][np.argmin(distances)] = 1 \n",
    "            \n",
    "            # Second phase, the values of the cluster means are computed\n",
    "            self.means = self.Z.T.dot(train_data) / np.sum(self.Z.T, axis=1).reshape(self.k, 1)\n",
    "            \n",
    "            n_iter += 1\n",
    "        \n",
    "    def assign_cluster(self, data):\n",
    "        \n",
    "        # Will hold the cluster that each data point belongs to\n",
    "        clusters = np.zeros(data.shape[0], dtype=int)\n",
    "        \n",
    "        for i, x in enumerate(data):            \n",
    "            distances = np.linalg.norm(self.means - x, axis=1)**2\n",
    "            clusters[i] = np.argmin(distances)\n",
    "        \n",
    "        return clusters\n",
    "            \n",
    "    \n",
    "    # Helper function that checks the convergence of the K-means algorithm\n",
    "    def _converged(self, old_Z, n_iter):\n",
    "        if n_iter == 0:\n",
    "            return False\n",
    "        elif np.array_equal(old_Z, self.Z):\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM algorithm for a Gaussian mixture with general covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EM_GMM:\n",
    "    \n",
    "    def __init__(self, k=4, n_dims=2):\n",
    "        self.k = k\n",
    "        self.n_dims = n_dims\n",
    "        \n",
    "    def train(self, train_data, means, clusters, MAX_ITER = 100):\n",
    "        # We start off by initializing our gaussian mixture parameters with the parameters given to us\n",
    "        self.means = means\n",
    "        self.covs = [np.eye(self.n_dims)] * self.k\n",
    "        \n",
    "        # compute the sample covariance of each cluster\n",
    "        for i in xrange(self.k):\n",
    "            self.covs[i] = np.cov(train_data[np.where(clusters==i)[0],:], rowvar=False) \n",
    "\n",
    "        # posterior probabilities or the weights N x K matrix\n",
    "        self.taus = np.zeros(shape=(train_data.shape[0], self.k))\n",
    "        self.pi = np.bincount(clusters) / clusters.shape[0]        \n",
    "        n_iter = 0\n",
    "        \n",
    "        while(n_iter < MAX_ITER):            \n",
    "            # E step\n",
    "            for i in xrange(self.k):\n",
    "                self.taus[:, i] = self.pi[i] * mv_normal.pdf(train_data, self.means[i], self.covs[i], True)\n",
    "            \n",
    "            # normalize the taus to get posterior probabilities\n",
    "            self.taus = (self.taus.T / np.sum(self.taus, axis=1)).T\n",
    "            \n",
    "            # M step            \n",
    "            # Compute the new means and covariance matrices\n",
    "            for i in xrange(self.k):\n",
    "                tau_sum = np.sum(self.taus[:, i])\n",
    "                \n",
    "                # First the mean for cluster i\n",
    "                self.means[i] = (np.sum(self.taus[:, i].reshape(self.taus.shape[0], 1) * train_data, axis=0) / tau_sum)\n",
    "                \n",
    "                distance = train_data - self.means[i]\n",
    "                self.covs[i] = (distance.T.dot(self.taus[:, i].reshape(self.taus.shape[0], 1) * distance) / tau_sum)\n",
    "                self.pi[i] = tau_sum / train_data.shape[0]\n",
    "\n",
    "                n_iter += 1\n",
    "    \n",
    "    def assign_cluster(self, data):\n",
    "        taus = np.zeros(shape=(data.shape[0], self.k))\n",
    "        for i in xrange(self.k):\n",
    "                taus[:, i] = self.pi[i] * mv_normal.pdf(data, self.means[i], self.covs[i], True)\n",
    "                \n",
    "        clusters = np.zeros(data.shape[0], dtype=int)\n",
    "        for i, x in enumerate(data):\n",
    "            clusters[i] = np.argmax(taus[i, :])\n",
    "        return clusters\n",
    "    \n",
    "    def normalized_log_likelihood(self, data):\n",
    "        like = np.zeros(shape=(data.shape[0], self.k))\n",
    "        for i in xrange(self.k):\n",
    "                like[:, i] = self.pi[i] * mv_normal.pdf(data, self.means[i], self.covs[i], True)\n",
    "        \n",
    "        loglike = np.log(np.sum(like, axis=1))\n",
    "        loglike = np.sum(loglike) / data.shape[0]\n",
    "        \n",
    "        return loglike\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
